# -*- coding: utf-8 -*-
"""nnls_dual.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tfS3MH1l05nqmLi9KCsWQd5pucfFtW3k
"""

"""
Nonnegative Least Squares (NNLS): Primal & Dual Solvers with KKT Verification

What this script does
- Solves min_x 0.5||Ax - y||^2  s.t. x >= 0 via:
    (a) Projected Gradient Descent (primal)
    (b) Dual projected gradient ascent on lambda >= 0
- Avoids explicit matrix inverses; uses Cholesky solves
- Compares against a CVXPY baseline (strong duality check)
- Prints feasibility, stationarity, and complementarity stats

How to run
    pip install -r requirements.txt
    python nnls_primal_dual_kkt.py
"""

import numpy as np
import cvxpy as cp
import scipy.linalg as la

# --------------------------
# Data
# --------------------------
np.random.seed(202503)
m, n = 40, 15
A = np.random.randn(m, n)
y = np.random.randn(m)

# Precompute normal equations factorization
AtA = A.T @ A
AtY = A.T @ y
# Regularize slightly for numerical safety
L = la.cholesky(AtA + 1e-10 * np.eye(n), lower=True)
def solve_AtA(b):
    # solves (AtA) x = b using cached Cholesky
    return la.cho_solve((L, True), b)

# --------------------------
# Helpers
# --------------------------
def project_nonneg(x):
    return np.maximum(x, 0.0)

def objective(x):
    r = y - A @ x
    return 0.5 * np.dot(r, r)

def grad_primal(x):
    return A.T @ (A @ x - y)

def spectral_norm_estimate(A, iters=50):
    """Power iteration to estimate ||A||_2^2 on A^T A."""
    z = np.random.randn(A.shape[1])
    z /= (np.linalg.norm(z) + 1e-12)
    for _ in range(iters):
        z = A.T @ (A @ z)
        zn = np.linalg.norm(z)
        if zn == 0:
            return 1.0
        z /= zn
    # Rayleigh quotient (approx largest eigenvalue of A^T A)
    lam = z @ (A.T @ (A @ z))
    return lam

# --------------------------
# Primal Projected Gradient Descent
# --------------------------
def projected_gradient_descent(A, y, tol=1e-6, max_iter=2000, alpha=None, x0=None):
    m, n = A.shape
    if alpha is None:
        Lhat = spectral_norm_estimate(A)  # ~ largest eigenvalue of A^T A
        alpha = 1.0 / (Lhat + 1e-12)
    x = np.zeros(n) if x0 is None else x0.copy()

    for k in range(max_iter):
        x_prev = x
        x = project_nonneg(x - alpha * grad_primal(x))
        if np.linalg.norm(x - x_prev) < tol:
            break
    return x

# --------------------------
# Dual Projected Gradient Ascent
# --------------------------
def dual_function_value(lam):
    # d(lam) = -0.5 lam^T (AtA)^{-1} lam - lam^T (AtA)^{-1} AtY + 0.5 y^T y
    v1 = solve_AtA(lam)
    v2 = solve_AtA(AtY)
    return -0.5 * lam @ v1 - lam @ v2 + 0.5 * y @ y

def dual_gradient(lam):
    # ∇d(lam) = -(AtA)^{-1} (AtY + lam)
    return -solve_AtA(AtY + lam)

def dual_projected_gradient_ascent(A, y, beta=None, tol=1e-6, max_iter=5000):
    # Use similar Lipschitz idea for dual step
    # Lipschitz for grad is related to ||(AtA)^{-1}||; we pick a conservative beta
    if beta is None:
        # crude: inverse of smallest eigenvalue ~ 1 / (lambda_min(AtA))
        # estimate lambda_max(AtA) first; for safety we take beta small
        beta = 0.01
    lam = np.zeros(A.shape[1])
    for k in range(max_iter):
        lam_prev = lam
        lam = project_nonneg(lam + beta * dual_gradient(lam))
        if np.linalg.norm(lam - lam_prev) < tol:
            break
    x_dual = solve_AtA(AtY + lam)
    return x_dual, lam

# --------------------------
# CVXPY baseline (for verification)
# --------------------------
def solve_cvxpy(A, y):
    n = A.shape[1]
    x_var = cp.Variable(n, nonneg=True)
    prob = cp.Problem(cp.Minimize(0.5 * cp.sum_squares(A @ x_var - y)))
    prob.solve(solver="OSQP", eps_abs=1e-9, eps_rel=1e-9, verbose=False)
    return x_var.value, prob.value

# --------------------------
# KKT checks and summary
# --------------------------
def kkt_report(x, lam):
    # KKT for NNLS:
    # 1) Primal feasibility: x >= 0
    # 2) Dual feasibility:  lam >= 0  where lam = gradient wrt x of Lagrangian at optimum
    # 3) Stationarity: A^T(Ax - y) - lam = 0
    # 4) Complementary slackness: lam_i * x_i = 0
    primal_feas = np.all(x >= -1e-10)
    dual_feas   = np.all(lam >= -1e-10)
    station     = grad_primal(x) - lam
    comp        = lam * x
    print("\nKKT CHECKS")
    print(f" Primal feasibility (x>=0): {primal_feas}")
    print(f" Dual feasibility   (λ>=0): {dual_feas}")
    print(f" Stationarity ||A^T(Ax - y) - λ||: {np.linalg.norm(station):.3e}")
    print(f" Complementarity max |λ_i x_i|: {np.max(np.abs(comp)):.3e}")
    print(f" Complementarity mean |λ_i x_i|: {np.mean(np.abs(comp)):.3e}")

# --------------------------
# Run all
# --------------------------
if __name__ == "__main__":
    x_pgd = projected_gradient_descent(A, y)
    f_pgd = objective(x_pgd)
    print(f"PGD objective: {f_pgd:.6f}")

    x_dual, lam_star = dual_projected_gradient_ascent(A, y)
    f_dual = objective(x_dual)
    print(f"Dual ascent objective: {f_dual:.6f}")

    x_cvx, f_cvx = solve_cvxpy(A, y)
    print(f"CVXPY objective: {f_cvx:.6f}")

    print(f"\nGaps (method - cvx): PGD: {f_pgd - f_cvx:.3e}  |  Dual: {f_dual - f_cvx:.3e}")
    print(f"PGD vs Dual solution L2 diff: {np.linalg.norm(x_pgd - x_dual):.3e}")
    print(f"PGD vs CVX  solution L2 diff: {np.linalg.norm(x_pgd - x_cvx):.3e}")
    print(f"Dual vs CVX solution L2 diff: {np.linalg.norm(x_dual - x_cvx):.3e}")

    # KKT using dual multipliers from dual ascent
    kkt_report(x_dual, lam_star)